{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQtpPIY7/o3Ohx5hq2tcUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajidiSahithi/Session_10/blob/main/S10_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojiPA3sxTbBg"
      },
      "outputs": [],
      "source": [
        "%pip install albumentations==0.4.6\n",
        "%pip install  torch_lr_finder\n",
        "%pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "metadata": {
        "id": "NZK-Qgu9VBg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "def get_a_train_transform():\n",
        "    \"\"\"Get transformer for training data\n",
        "\n",
        "    Returns:\n",
        "        Compose: Composed transformations\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.Normalize(\n",
        "            mean = (0.4914, 0.4822, 0.4465),\n",
        "            std = (0.2470, 0.2435, 0.2616), always_apply = True\n",
        "        ),\n",
        "        A.PadIfNeeded(min_height=36, min_width=36, always_apply = True),\n",
        "        A.RandomCrop(height=32, width=32, always_apply = True),\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.CoarseDropout(max_holes = 1, max_height=8, max_width=8, min_holes = 1, min_height=8, min_width=8,\n",
        "                        fill_value=(0.4914, 0.4822, 0.4465), always_apply = True),\n",
        "        ToTensorV2()\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "def get_a_test_transform():\n",
        "    \"\"\"Get transformer for test data\n",
        "\n",
        "    Returns:\n",
        "        Compose: Composed transformations\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "         A.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010), always_apply = True),\n",
        "                                  ToTensorV2()\n",
        "    ])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bwifW0d70_lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "class Cifar10SearchDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset Class\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, transforms=None):\n",
        "        \"\"\"Initialize Dataset\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): Pytorch Dataset instance\n",
        "            transforms (Transform.Compose, optional): Tranform function instance. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get dataset length\n",
        "\n",
        "        Returns:\n",
        "            int: Length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get an item form dataset\n",
        "\n",
        "        Args:\n",
        "            idx (int): id of item in dataset\n",
        "\n",
        "        Returns:\n",
        "            (tensor, int): Return tensor of transformer image, label\n",
        "        \"\"\"\n",
        "        # Read Image and Label\n",
        "        image, label = self.dataset[idx]#,self.targets[idx]\n",
        "\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Apply Transforms\n",
        "        if self.transforms is not None:\n",
        "            image = (self.transforms(image=image))[\"image\"]\n",
        "\n",
        "\n",
        "        return (image, label)\n",
        "\n",
        "\n",
        "def get_loader(train_transform, test_transform, batch_size=128, use_cuda=True):\n",
        "    \"\"\"Get instance of tran and test loaders\n",
        "    self.batch_size = 128\n",
        "        self.device = device\n",
        "        self.use_cuda = use_cuda\n",
        "    Args:\n",
        "        train_transform (Transform): Instance of transform function for training\n",
        "        test_transform (Transform): Instance of transform function for validation\n",
        "        batch_size (int, optional): batch size to be uised in training. Defaults to 64.\n",
        "        use_cuda (bool, optional): Enable/Disable Cuda Gpu. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        (DataLoader, DataLoader): Get instance of train and test data loaders\n",
        "    \"\"\"\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Cifar10SearchDataset(datasets.CIFAR10('../data', train=True,\n",
        "                     download=True), transforms=train_transform),\n",
        "        batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        Cifar10SearchDataset(datasets.CIFAR10('../data', train=False,\n",
        "                     download=True), transforms=test_transform),\n",
        "        batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "kd9lkknu1AT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "use_cuda=True\n",
        "train_loader, test_loader = get_loader(get_a_train_transform(), get_a_test_transform(), batch_size=batch_size, use_cuda=use_cuda)\n",
        "\n",
        "\n",
        "print(get_a_train_transform())\n",
        "\n",
        "print(get_a_test_transform())\n",
        "\n",
        "print(train_loader)\n",
        "print('length of train_loader',len(train_loader))\n",
        "print(test_loader)\n",
        "print('length of test_loader',len(test_loader))\n"
      ],
      "metadata": {
        "id": "7n109X_g1CNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "def print_samples(loader, count=16):\n",
        "    \"\"\"Print samples input images\n",
        "\n",
        "    Args:\n",
        "        loader (DataLoader): dataloader for training data\n",
        "        count (int, optional): Number of samples to print. Defaults to 16.\n",
        "    \"\"\"\n",
        "    # Print Random Samples\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    if not count % 8 == 0:\n",
        "        return\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    for imgs, labels in loader:\n",
        "        for i in range(count):\n",
        "            ax = fig.add_subplot(int(count/8), 8, i + 1, xticks=[], yticks=[])\n",
        "            ax.set_title(f'{classes[labels[i]]}')\n",
        "            plt.imshow(imgs[i].numpy().transpose(1, 2, 0))\n",
        "        break\n"
      ],
      "metadata": {
        "id": "146jP2d01Eaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_samples(train_loader)"
      ],
      "metadata": {
        "id": "kR5RIGuf1Gkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        drop=0.01\n",
        "        # Preparation Layer\n",
        "        self.conv1 = nn.Sequential (\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )  # Number of Parameters = 3*3*3*64=1728\n",
        "        # Layer 1\n",
        "        self.conv11 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,bias=False),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )  # Number of Parameters = 3*3*64*128 = 73728\n",
        "        self.conv12 = nn.Sequential(\n",
        "            nn.Conv2d(128,128, kernel_size=3, stride=1, padding=1,bias=False),# Number of Parameters = 3*3*64*128 = 73728\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128,128, kernel_size=3, stride=1, padding=1,bias=False),# Number of Parameters = 3*3*64*128 = 73728\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1,bias=False),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        # Layer 3\n",
        "        self.conv31 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1,bias=False),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )\n",
        "        self.conv32 = nn.Sequential(\n",
        "            nn.Conv2d(512,512, kernel_size=3, stride=1, padding=1,bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512,512, kernel_size=3, stride=1, padding=1,bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=4,stride=2)\n",
        "\n",
        "        # Fully connected\n",
        "        self.fc = nn.Linear(512, 10, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.conv11(x)\n",
        "        R1=x\n",
        "        x = self.conv12(x)\n",
        "        x=x+R1\n",
        "\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.conv31(x)\n",
        "        R2=x\n",
        "        x = self.conv32(x)\n",
        "        x=x+R2\n",
        "\n",
        "        x = self.maxpool(x)\n",
        "        #x = x.randn(512, 1)\n",
        "\n",
        "# squeeze the tensor to size 512x\n",
        "        x = x.squeeze(dim=[2, 3])\n",
        "\n",
        "        #x = x.view(512, 10)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        #return x\n",
        "        y = F.log_softmax(x, dim=-1)\n",
        "        return  y\n",
        "\n",
        "def model_summary(model,input_size):\n",
        "    model = Net().to(device)\n",
        "    summary(model, input_size=(3, 32, 32))\n",
        "    return model,input_size"
      ],
      "metadata": {
        "id": "a_9JczAR1Ixd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "id": "WcU0j5Qn1K_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "RELOfXQB1OUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, epoch):\n",
        "    train_losses = []\n",
        "    #test_losses = []\n",
        "    train_acc = []\n",
        "    #test_acc = []\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "        optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "        y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "        loss = F.nll_loss(y_pred, target)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "        pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)\n",
        "\n",
        "def test_model(model, device, test_loader):\n",
        "    #train_losses = []\n",
        "    test_losses = []\n",
        "    #train_acc = []\n",
        "    test_acc = []\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "cn5xA0m11Pd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch_lr_finder import LRFinder\n",
        "\n",
        "model =  Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.03, weight_decay=1e-4)\n",
        "\n",
        "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
        "lr_finder.range_test(train_loader, end_lr=10, num_iter=200, step_mode=\"exp\")\n",
        "lr_finder.plot() # to inspect the loss-learning rate graph\n",
        "lr_finder.reset() # to reset the model and optimizer to their initial state\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_fWz9mY1RqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "EPOCHS = 24\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=4.65E-02\n",
        ",\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=EPOCHS,\n",
        "        pct_start=5/EPOCHS,\n",
        "        div_factor=100,\n",
        "        three_phase=False,\n",
        "        final_div_factor=100,\n",
        "        anneal_strategy='linear'\n",
        "    )\n",
        "EPOCHS = 24\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train_model(model, device, train_loader, optimizer, epoch)\n",
        "    # scheduler.step()\n",
        "    test_model(model, device, test_loader)"
      ],
      "metadata": {
        "id": "Hp3bXSvo1aAA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}